{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"cfds_logo.png\">\n",
    "\n",
    "###  Lab 05 - \"Supervised Machine Learning - Naive-Bayes Classification\"\n",
    "\n",
    "Chartered Financial Data Scientist (CFDS), Autumn Term 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this fifth lab, we will build our first **supervised machine learning classification \"pipelines\"** using a classifier named the **Gaussian Naive-Bayes (GNB)** classifier. \n",
    "\n",
    "The *generative* **Naive-Bayes (NB)** classifier belongs to the family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with a strong (naive) independence assumptions between the features. Naive Bayes has been studied extensively since the 1950s and remains an accessible (baseline) method for text categorization as well as other domains. \n",
    "\n",
    "This classification technique is part of the **generative** type of classifiers, which can be distinguished from the **discriminative** type as shown by the following illustration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"supervisedlearning.png\">\n",
    "\n",
    "(Inspired by: 'Machine Learning - A Probabilistic Perspective', Kevin P. Murphy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our NextThought lab discussion forum (https://financial-data-science.nextthought.io), or send us an email (using our fds.ai email addresses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Know how to setup a **notebook or \"pipeline\"** that solves a simple supervised classification task.\n",
    "> 2. Recognize the distinct **data elements** (features and labels) needed to train and evaluate a supervised machine learning classifier. \n",
    "> 3. Understand how a Gaussian **Naive-Bayes (NB)** classifier can be trained and evaluated.\n",
    "> 4. Know how to use Python's sklearn library to **train** and **evaluate** arbitrary classifiers.\n",
    "> 5. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# OpenAI: \"Solving Rubik's Cube with a Robot Hand\"\n",
    "# YouTubeVideo('x4O8pojMF0w', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the Analysis Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. In this lab will use the `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` and the `Seaborn` library. Let's import the libraries by the execution of the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import sklearn naive.bayes and k-nearest neighbor classifier library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 'Seaborn' plotting style in all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed of all our experiments - this insures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gaussian \"Naive-Bayes\" (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Iris Dataset** is a classic and straightforward dataset often used as a \"Hello World\" example in multi-class classification. This data set consists of measurements taken from three different types of iris flowers (referred to as **Classes**),  namely the Iris Setosa, the Iris Versicolour and the Iris Virginica, and their respective measured petal and sepal length (referred to as **Features**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"iris_dataset.png\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the dataset consists of **150 samples** (50 samples taken per class) as well as their corresponding **4 different measurements** taken for each sample. Please, find below the list of the individual measurements:\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details of the dataset can be obtained from the following puplication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the names of the four features contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the class label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the names of the three classes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly envision how the feature information of the dataset is collected and presented in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"featurecollection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top ten feature rows of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data, columns=iris.feature_names).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the top ten class labels of the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target, columns=[\"class\"]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now conduct a more in depth data assessment. Therefore, we plot the feature distributions of the Iris dataset according to their respective class memberships as well as the features pairwise relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pls. note that we use Python's **Seaborn** library to create such a plot referred to as **Pairplot**. The Seaborn library is a powerful data visualization library based on the Matplotlib. It provides a great interface for drawing informative statstical graphics (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed from the created Pairplot, that most of the feature measurements that correspond to flower class \"setosa\" exhibit a nice **linear seperability** from the feature measurements of the remaining flower classes. In addition, the flower classes \"versicolor\" and \"virginica\" exhibit a commingled and **non-linear seperability** across all the measured feature distributions of the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice, to divide the dataset into a **training set** (the fraction of data records solely used for training purposes) and a **evaluation set** (the fraction of data records solely used for evaluation purposes). Pls. note, the **evaluation set** will never be shown to the model as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"trainevaldataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the fraction of evaluation records to **30%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split the dataset into training set and evaluation set using sklearn's `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(iris.data, iris.target, test_size=eval_fraction, random_state=random_seed, stratify=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training set dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the evaluation set dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Gaussian Naive-Bayes (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One popular (and remarkably simple) algorithm is the **Naive Bayes Classifier**. Note, that one natural way to adress a given classification task is via the probabilistic question: **\"What is the most likely class $c^{*}$ considering all the available information $x$?\"** Formally, we wish to output a conditional probability $P(c|x)$ for each class $c$ given distinct observations of $x$. Once we obtained such conditional probability for each class we select the class $c^{*}$ corresponding to the highest $P(c|x)$ as expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c^{*} = \\arg \\max_{c} P(c|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That would require that we need to be prepared to estimate the probability distribution $P(c | \\mathbf{x})$ for every possible value of $\\mathbf{x} = \\{x_1, x_2, ..., x_n\\}$. \n",
    "\n",
    "**Excursion:** Imagine a document classification system that, depending on the occurance of a particular set of words in a document, predicts the class of the document. For example, if a the words **\"recipe\"**, **\"pumpkin\"**, **\"cuisine\"**, **\"pancakes\"**, etc. appear in the document, the classifier predicts a high probability of the document beeing a cookbook. Let's assume that the feature $x_{pancake} = 1$ might signify that the word **\"pancakes\"** appears in a given document and $x_{pancake} = 0$ would signify that it does not. If we had **30** such binary **\"word-appearence\" features**, that would mean that we need to be prepared to calculate the probability $P(c | \\mathbf{x})$ of any of $2^{30}$ (over 1 billion) possible values of the input vector $\\mathbf{x}= \\{x_1, x_2, ..., x_{30}\\}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{x^{1}}= \\{x_1=1, x_2=0, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{2}}= \\{x_1=1, x_2=1, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{3}}= \\{x_1=1, x_2=1, x_3=1, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$...$$\n",
    "$$...$$\n",
    "$$\\mathbf{x^{2^{30}-1}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=0, x_{30}=1\\}$$\n",
    "$$\\mathbf{x^{2^{30}}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=1, x_{30}=1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, where is the learning? If we need to see every single possible example in order to predict the corresponding label then we're not really learning a pattern but just memorizing the dataset. One solution to this challenge is the so-called **Bayes' theorem** (alternatively Bayes' law or Bayes' rule) that you learned about in the lecture. A common scenario for applying the Bayes' theorem formula is when you want to know the probability of something ‚Äúunobservable‚Äù (e.g., the class $c$ of a document) given an ‚Äúobserved‚Äù event (e.g., the distinct words $x$ contained in the document). Such a probability is usually referred to as **posterior probability** mathematically denoted by $P(c|x)$.\n",
    "\n",
    "The Bayes' theorem provides an elegant way of calculating such posterior probabilities $P(c|x)$ without the need of observing every single possible configuration of $\\mathbf{x} = \\{x_1, x_2, ..., x_n\\}$. Let's briefly revisit the formula of the Bayes' theorem below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"bayestheorem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the formula of the **Bayes' theorem** above,\n",
    "\n",
    ">- $P(c|x)$ denotes the **posterior** probability of class $c$ given a set of features $x$ denoted by $x_1, x_2, ..., x_n$.\n",
    ">- $P(c)$ denotes the **prior** probability of observing class $c$.\n",
    ">- $P(x|c)$ denotes the **likelihood** which is the probability of a feature $x$ given class $c$.\n",
    ">- $P(x)$ denotes the **evidence** which is the general probability of observing feature $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1. Calculation of the prior probabilities $P(c)$ of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an intuition of the Bayes' theorem by first calculating the prior probability $P(c)$ of each class iris flower contained in the dataset. Therefore, we first obtain the number of occurance of each class in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine counts of unique class labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# concatenate counts and class labels in a python dictionary\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# print obtained dictionary\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the obtained counts into probabilites. Therefore, we divide the class counts by the overall number of observations contained in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide counts by the number of observations available in the training data\n",
    "prior_probabilities = counts / np.sum(counts)\n",
    "\n",
    "# print obtained probabilites\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the obtained prior probabilites $P(c)$ accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.bar(x=np.unique(iris.target), height=prior_probabilities, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$c_{i}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(c_{i})$\", fontsize=14)\n",
    "\n",
    "# set x-axis ticks\n",
    "ax.set_xticks(np.unique(iris.target))\n",
    "\n",
    "# set y-axis range\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the Prior Class Probabilites $P(c)$', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2. Calculation of the evidence $P(x)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the general probability of observing a particular observation $ùë•$ which from A Bayes' theorem perspective denotes the evidence $P(\\mathbf{x})$ of an observation $x=\\{x_1, x_2, ..., x_n\\}$. We assume that the first feature $x_{1}$ represents the \"sepal length\" observations of the Iris Dataset, the second feature $x_{2}$ = \"sepal width\", $x_{3}$ = \"petal length\", and $x_{4}$ = \"petal width\". \n",
    "\n",
    "In order to calculate the evidence $P(x)$ of a particular observation, e.g, $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$ the Bayes' theorem in general utilizes the following two tricks:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"** \n",
    "\n",
    "Using the **\"Chain Rule of Probabilities\"**, we can express the evidence term $P( \\mathbf{x} )$ as:\n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2 | x_1) \\cdot P(x_3 | x_1, x_2) \\cdot P(x_4 | x_1, x_2, x_3) \\cdot ... \\cdot P( x_n | x_1, ..., x_{n-1}) = \\prod^n_i P(x_i | x_{1:i-1})$$\n",
    "\n",
    "By itself, this expression doesn't get us any further. We still need, even in a case of $d$ binary features, to estimate roughly $2^d$ parameters. The trick of the **naive** Bayes' theorem however is to assume that the distinct features $x_1, x_2, ..., x_n$ are **conditionally independent** from each other when observing a particular class $c$. Using this assumption we're in much better shape, as the evidence term $P(\\mathbf{x})$ simplifies to: \n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2) \\cdot P(x_3) \\cdot P(x_4) \\cdot ... \\cdot P( x_n ) = \\prod^n_i P(x_i)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i)$ amounts to estimating the distribution of each feature $x_i$ independently. As a result, the assumption of conditional independence reduced the complexity of our model (in terms of the number of parameters) from an exponentially growing dependence in the number of features to a linear growing dependence. Hence, we call it the **\"naive\"** Bayes' theorem, since it makes the naive assumption about feature independence, so we don't have to care about dependencies among them.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"** \n",
    "\n",
    "During the lecture you learned that evidence distribution can be approximated by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. This simplification can be justified by the application of the **\"Law of Large Numbers\"** or **\"Central Limit Theorem\"** (you may want to have a look at further details of the theorem under: https://en.wikipedia.org/wiki/Central_limit_theorem). In general, the probability density of a Gaussian \"Normal\" distribution, as defined by the formula below. It is parametrized its **mean $\\mu$** and corresponding **standard deviation $\\sigma$**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"evidencecalculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **\"Law of Large Numbers\"** we will approximate the evidence probability density $P(x) \\approx \\mathcal{N}(x | \\mu, \\sigma)$ of each of each feature $x_i$ by a Gaussian. To achieve this we need to come up with a good estimate of the parameters $\\mu$ and $\\sigma$ that define a Gaussian (Normal) probability distribution.\n",
    "\n",
    "But how can this be achieved in practice? Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the Iris Dataset. The following line of code determines a histogram of the true **sepal length** feature value distribution and plots it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution\n",
    "hist_probabilities, hist_edges = np.histogram(x_train[:, 0], bins=10, range=(0,10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_probabilities)\n",
    "\n",
    "# print the histogram edges\n",
    "print(hist_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1})$\", fontsize=14)\n",
    "\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" Feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we approximate the true probability density of the **sepal length** feature using a Gaussian distribution? Well, all we need to do is to calculate it's mean $\\mu$ and standard deviation $\\sigma$. Let's start by calculating the mean $\\mu$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations\n",
    "mean_sepal_length = np.mean(x_train[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by calculating the standard devition $\\sigma$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations\n",
    "std_sepal_length = np.std(x_train[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now determine the approximate Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu, \\sigma)$ of the **sepal length** feature using the $\\mu$ and $\\sigma$ obtained above. Thereby, we will utilize the `pdf.norm` function available in the `scipy.stats` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length, std_sepal_length)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1})$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's likewise approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$ of the **sepal width** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature\n",
    "mean_sepal_width = np.mean(x_train[:, 1])\n",
    "std_sepal_width = np.std(x_train[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width, std_sepal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal width\" observations\n",
    "ax.hist(x_train[:, 1], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{2}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{2})$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" Feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$ of the **petal length** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature\n",
    "mean_petal_length = np.mean(x_train[:, 2])\n",
    "std_petal_length = np.std(x_train[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length, std_petal_length), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal length\" observations\n",
    "ax.hist(x_train[:, 2], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{3}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{3})$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" Feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$ of the **petal width** feature and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature\n",
    "mean_petal_width = np.mean(x_train[:, 3])\n",
    "std_petal_width = np.std(x_train[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width, std_petal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal width\" observations\n",
    "ax.hist(x_train[:, 3], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{4}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{4})$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" Feature', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Calculation of the likelihood $P(x|c)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how we can calculate the **likelihood** $P(\\mathbf{x}|c)$ which is the probability density of a feature given a particular class $c$. We will again make use of the two tricks that we applied when calculating the **evidence** $P(x)$ probabilities. In order to calculate the likelihood $P(x|c)$ of a particular observation, e.g, $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5 | c=\"setosa\"\\}$ we will apply:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"**, using the **\"Chain Rule of Probabilities\"**, we can express the likelihood term $P( \\mathbf{x} | c)$ as:\n",
    "\n",
    "$$P( \\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1, c) \\cdot P(x_2 | x_1, c) \\cdot P(x_3 | x_1, x_2, c) \\cdot P(x_4 | x_1, x_2, x_3, c) \\cdot ... \\cdot = \\prod^n_i P(x_i | x_{1:i-1}, c)$$\n",
    "\n",
    "We will again assume that the distinct features $x_1, x_2, ..., x_n$ are conditionally independent from each other when observing a particular class $c$. As a result the likelihood term $P( \\mathbf{x} | c)$ simplifies to: \n",
    "\n",
    "$$P( \\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1 | c) \\cdot P(x_2 | c) \\cdot P(x_3 | c) \\cdot P(x_4 | c) \\cdot ... \\cdot P( x_n | c) = \\prod^n_i P(x_i | c)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i | c)$ amounts to estimating the distribution of each feature $x_i$ independently.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"**, using this simplification we can can estimate $P(\\mathbf{x}|c)$ by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. The **likelihood** probability density of a Gaussian \"Normal\" distribution, as defined by the formula below, is determined by its mean $\\mu$, standard deviation $\\sigma$ and it's corresponding class condition $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"likelihoodcalculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **\"Law of Large Numbers\"** we will approximate the likelihood probability density $P(x | c) \\approx \\mathcal{N}(x | \\mu, \\sigma, c)$ of each of each feature $x_i$ by a Gaussian. To achieve this we need to come up with a good estimate of the parameters $\\mu$ and $\\sigma$ that define a Gaussian (Normal) probability distribution.\n",
    "\n",
    "But how can this be achieved in practice? Let's start by applying the class conditioning. This is usually done by filtering the dataset for each class $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all iris setosa measurements, class label = 0\n",
    "x_train_setosa = x_train[y_train == 0]\n",
    "\n",
    "# collect all iris versicolor measurements, class label = 1\n",
    "x_train_versicolor = x_train[y_train == 1]\n",
    "\n",
    "# collect all iris virginica measurements, class label = 2\n",
    "x_train_virginica = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by inspecting the true probability density of the **sepal length** feature (the first feature) of the iris dataset given the class **setosa**. The following line of code determines a histogram of the true feature value distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution given the class \"setosa\"\n",
    "hist_setosa, bin_edges_setosa = np.histogram(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_setosa)\n",
    "\n",
    "# print the histogram edges\n",
    "print(bin_edges_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1}|c=setosa)$\", fontsize=14)\n",
    "ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" Feature given Class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are again able to determine the approximate Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal length** feature given the class **setosa** using the $\\mu$ and $\\sigma$ obtained above as well as the `pdf.norm` function of the `scipy.stats` package.\n",
    "\n",
    "Let's continue by calculating the mean $\\mu$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations given class \"setosa\"\n",
    "mean_sepal_length_setosa = np.mean(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue by calculating the standard devition $\\sigma$ of the **sepal length** feature given the class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations given class \"setosa\"\n",
    "std_sepal_length_setosa = np.std(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length_setosa = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length_setosa, std_sepal_length_setosa)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal length** feature given class **setosa**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length_setosa, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{1}|c=setosa)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" Feature given Class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's likewise approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal width** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature given class setosa\n",
    "mean_sepal_width_setosa = np.mean(x_train_setosa[:, 1])\n",
    "std_sepal_width_setosa = np.std(x_train_setosa[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width_setosa, std_sepal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 1], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{2}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{2}|c=setosa)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" Feature given Class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **petal length** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature given class setosa\n",
    "mean_petal_length_setosa = np.mean(x_train_setosa[:, 2])\n",
    "std_petal_length_setosa = np.std(x_train_setosa[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length_setosa, std_petal_length_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 2], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{3}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{3}|c=setosa)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" Feature given Class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **petal width** feature given class **setosa** and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature given class setosa\n",
    "mean_petal_width_setosa = np.mean(x_train_setosa[:, 3])\n",
    "std_petal_width_setosa = np.std(x_train_setosa[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width_setosa, std_petal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 3], bins=10, range=(0, 10), density=True, color='darkred')\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{4}$\", fontsize=14)\n",
    "ax.set_ylabel(\"$P(x_{4}|c=setosa)$\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" Feature given Class \"Setosa\"', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now also compute mean $\\mu$ and standard deviation $\\sigma$ of the feature value distribution that correspond to the **'versicolor'** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'versicolor'\n",
    "mean_sepal_length_versicolor = np.mean(x_train_versicolor[:, 0])\n",
    "std_sepal_length_versicolor = np.std(x_train_versicolor[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'versicolor'\n",
    "mean_sepal_width_versicolor = np.mean(x_train_versicolor[:, 1])\n",
    "std_sepal_width_versicolor = np.std(x_train_versicolor[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'versicolor'\n",
    "mean_petal_length_versicolor = np.mean(x_train_versicolor[:, 2])\n",
    "std_petal_length_versicolor = np.std(x_train_versicolor[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'versicolor'\n",
    "mean_petal_width_versicolor = np.mean(x_train_versicolor[:, 3])\n",
    "std_petal_width_versicolor = np.std(x_train_versicolor[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the mean $\\mu$ and standard deviation $\\sigma$ of the feature value distribution that correspond to the **'virginica'** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'virginica'\n",
    "mean_sepal_length_virginica = np.mean(x_train_virginica[:, 0])\n",
    "std_sepal_length_virginica = np.std(x_train_virginica[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'virginica'\n",
    "mean_sepal_width_virginica = np.mean(x_train_virginica[:, 1])\n",
    "std_sepal_width_virginica = np.std(x_train_virginica[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'virginica'\n",
    "mean_petal_length_virginica = np.mean(x_train_virginica[:, 2])\n",
    "std_petal_length_virginica = np.std(x_train_virginica[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'virginica'\n",
    "mean_petal_width_virginica = np.mean(x_train_virginica[:, 3])\n",
    "std_petal_width_virginica = np.std(x_train_virginica[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3. Calculation of the posterior probability $P(x|c)$ of unknown iris flower observations $x^{s}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have determined all the distinct elements $P(c)$, $P(x)$ and $P(x|c)$ of the Bayes' theorem the determine the posterior probability $P(c=setosa|x)$ of a so far unseen \"new\" observations x of class **setosa**. Let's therefore determine if two so far unseen **iris flower** observations correspond to class **setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"iris_sample_1.png\">\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first **iris flower** observation $x^{s1}$ exhibits the following observed feature values: $x^{s1} = \\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init features of first iris flower observation \n",
    "sepal_length_sample_1 = 5.8 \n",
    "sepal_width_sample_1  = 3.5\n",
    "petal_length_sample_1 = 1.5\n",
    "petal_width_sample_1  = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build an intuition of the distinct iris flower class distributions including the current iris flower observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observation to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([[5.8, 3.5, 1.5, 0.25, \"observation 1\"]], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the posterior probability $P(c=setosa|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0]\n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length_sample_1, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width_sample_1, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length_sample_1, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width_sample_1, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length_sample_1, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width_sample_1, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length_sample_1, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width_sample_1, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length_sample_1, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width_sample_1, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length_sample_1, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width_sample_1, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x) = P(x|c='setosa') * P(c='setosa') + P(x|c='versicolor') * P(c='versicolor') + P(x|c='virginica') * P(c='virginica')\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our observed iris flower results in a posterior probability $P(c=setosa|x^{s1})$ of beeing of class setosa of 27.99. For comparison purposes, let's also determine the posterior probability $P(c=versicolor|x^{s1})$ and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the posterior probability $P(c=virginica|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the obtained posterior probabilites $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$:\n",
    "\n",
    "$$P(c=setosa|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{0.99}$$\n",
    "$$P(c=versicolor|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{4.69e^{-14}}$$\n",
    "$$P(c=virginica|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{2.20e^{-21}}$$\n",
    "\n",
    "we can now apply our initial classification criteria, denoted by $\\arg \\max_{c} P(c|x)$ to safely determine the observation's most likely class $c^{*} = setosa$.\n",
    "\n",
    "Let's now have a look at a second **iris flower** observation and determine its most likely class $c^{*}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"iris_sample_2.png\">\n",
    "\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second **iris flower** observation $x^{s2}$ exhibits the following observed feature values: $x^{s2} = \\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init a second random feature observation \n",
    "sepal_length_sample_2 = 7.8\n",
    "sepal_width_sample_2  = 2.3\n",
    "petal_length_sample_2 = 6.4\n",
    "petal_width_sample_2  = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again build an intuition of the distinct iris flower class distributions including the current iris flower observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observations to the iris dataset\n",
    "iris_plot = iris_plot.append(pd.DataFrame([[7.8, 2.3, 6.4, 2.50, \"observation 2\"]], columns=iris_plot.columns))\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the posterior probability $P(c=setosa|x^{s2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0] \n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length_sample_2, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width_sample_2, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length_sample_2, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width_sample_2, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length_sample_2, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width_sample_2, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length_sample_2, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width_sample_2, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length_sample_2, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width_sample_2, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length_sample_2, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width_sample_2, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x) = P(x|c='setosa') * P(c='setosa') + P(x|c='versicolor') * P(c='versicolor') + P(x|c='virginica') * P(c='virginica')\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our observed iris flower results in a very low posterior probability $P(c=setosa|x^{s2})$ of beeing of class setosa of $5.02e^{-268}$. For comparison purposes, let's also determine the posterior probability $P(c=versicolor|x^{s2})$ and see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the posterior probability $P(c=virginica|x^{s2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the obtained posterior probabilites $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:\n",
    "\n",
    "$$P(c=setosa|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.24e^{-268}}$$\n",
    "$$P(c=versicolor|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.12e^{-12}}$$\n",
    "$$P(c=virginica|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{0.99}$$\n",
    "\n",
    "we can now apply our initial classification criteria, denoted by $\\arg \\max_{c} P(c|x)$ to savely determine the observations most likely class $c^{*} = virginica$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4. Training and utilization of a Gaussian Naive-Bayes Classifier using Python's Sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there is a Python library named `Scikit-Learn` (https://scikit-learn.org) that provides a variety of machine learning algorithms that can be easily interfaced using the Python programming language. It also contains supervised classification algorithms such as the **Gaussian Naive-Bayes** classifier which we can use of the shelf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, for each classifier, available in the `Scikit-Learn` library, a designated and detailed documentation is provided. It often also includes a couple of practical examples and use cases. The documentation of the **Gaussian Naive-Bayes** classifer can be obtained from the following url: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `Scikit-Learn` and instantiate the **Gaussian Naive-Bayes** classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB(priors=None) # prior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or fit the Gaussian Naive-Bayes classifier using the training dataset features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Gaussian Naive Bayes classifier\n",
    "gnb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the trained model to predict the classes of the distinct observations contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the class labels **predicted** by the Gaussian Naive-Bayes classifier on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the **true** class labels as contained in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine number of **missclassified** data sampels in the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of mislabeled points out of a total {} points: {}\".format(x_eval.shape[0], np.sum(y_eval != y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of machine learning and in particular the field of statistical classification, a **confusion matrix**, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the number of instances that the classifier predicted per class, while each column represents the instances of the true or actual class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 300px; height: auto\" src=\"confusionmatrix.png\">\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]', fontsize=14)\n",
    "plt.ylabel('[predicted label]', fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Confusion Matrix - Gaussian Naive Bayes', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the learned model and apply it to our unknown observations $x^{s1}$ and $x^{s2}$ to determine their corresponding class predictions $c^{*}$:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine class label prediction of the first unknown observation\n",
    "class_prediction_sample_1 = gnb.predict([[5.8, 3.5, 1.5, 0.25]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_1[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine class label prediction of the second unknown observation\n",
    "class_prediction_sample_2 = gnb.predict([[7.8, 2.3, 6.4, 2.50]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train and evaluate the prediction accuracy of different train- vs. eval-data ratios.**\n",
    "\n",
    "> Change the ratio of training data vs. evaluation data to 30%/70% (currently 70%/30%), fit your model and calculate the new classification accuracy. Subsequently, repeat the experiment a second time using a 10%/90% fraction of training data/evaluation data. What can be observed in both experiments in terms of classification accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Calculate the true-positive as well as the false-positive rate of the Iris versicolor vs. virginica.**\n",
    "\n",
    "> Calculate the true-positive rate as well as false-positive rate of (1) the experiment exhibiting a 30%/70% ratio of training data vs. evaluation data and (2) the experiment exhibiting a 10%/90% ratio of training data vs. evaluation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, a step by step introduction into **Gaussian Naive-Bayes (GNB)** classification is presented. The code and exercises presented in this lab may serve as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment, e.g. on a compute node or a server. The cell below converts the lab notebook into a standalone and executable python script. Please note that to convert the notebook, you need to install Python's **nbconvert** library and its extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the nbconvert library\n",
    "!pip install nbconvert\n",
    "!pip install jupyter_contrib_nbextensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now convert the Jupyter notebook into a plain Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script cfds_lab_05a.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208.50543212890625px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
